{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Review of a Minimal, SB3-compatible Environment `rampup-v1` with A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import A2C, PPO\n",
    "from stable_baselines3.common.cmd_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from plan_opt.demand import Demand\n",
    "from plan_opt.demand_small_samples import four_weeks_uprising\n",
    "from plan_opt.envs.rampup1 import RampupEnv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "Demand is created deterministically from a hand-crafted blueprint of just four weeks of data for a fleet of size 1.\n",
    "The action space is descrete, only categorical changes of equipment are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "demand = Demand(period=len(four_weeks_uprising), data=four_weeks_uprising)\n",
    "demand.show(only_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altough the environment is registered with Gym as 'rampup-v1', it is imported straight from the module here. See notebook 05 using the registration with Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RampupEnv1(demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard logs saved with suffix 20k\n"
     ]
    }
   ],
   "source": [
    "algorithm = \"A2C\"\n",
    "timesteps = 20000\n",
    "tensorboard_log = \"logs/rampup_tensorboard/\"\n",
    "tb_log_suffix = f\"{str(timesteps)[:-3]}k\"\n",
    "print(f\"Tensorboard logs saved with suffix {tb_log_suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to logs/rampup_tensorboard/A2C_train_run_20k_2\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1289      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.31     |\n",
      "|    explained_variance | -6.94e+06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -2.66e+03 |\n",
      "|    value_loss         | 1.16e+07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1172     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.825   |\n",
      "|    explained_variance | -1.6e+07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 7.53e+03 |\n",
      "|    value_loss         | 6.12e+08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1156      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.714    |\n",
      "|    explained_variance | -1.32e+07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 1.86e+04  |\n",
      "|    value_loss         | 1.13e+09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1178      |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.19     |\n",
      "|    explained_variance | -1.64e+05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -6.69e+03 |\n",
      "|    value_loss         | 4.36e+07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1170      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.971    |\n",
      "|    explained_variance | -4.71e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -3.26e+03 |\n",
      "|    value_loss         | 2.64e+07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1167      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.777    |\n",
      "|    explained_variance | -4.72e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 9.92e+03  |\n",
      "|    value_loss         | 1.28e+08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1160      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.697    |\n",
      "|    explained_variance | -4.26e+08 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 1.34e+04  |\n",
      "|    value_loss         | 3.37e+08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1166     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.699   |\n",
      "|    explained_variance | -4.5e+08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 3.9e+03  |\n",
      "|    value_loss         | 6.87e+08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1177      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.1      |\n",
      "|    explained_variance | -2.28e+08 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -7.5e+03  |\n",
      "|    value_loss         | 3.07e+07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1173      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.733    |\n",
      "|    explained_variance | -9.21e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 5.6e+03   |\n",
      "|    value_loss         | 8.6e+07   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1181      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.505    |\n",
      "|    explained_variance | -2.18e+05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 465       |\n",
      "|    value_loss         | 2.63e+08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1184      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.767    |\n",
      "|    explained_variance | -7.94e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -2.83e+03 |\n",
      "|    value_loss         | 8.26e+06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1176      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.332    |\n",
      "|    explained_variance | -1.15e+08 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 3.3e+03   |\n",
      "|    value_loss         | 6.62e+08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1175      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.328    |\n",
      "|    explained_variance | -3.09e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -35.7     |\n",
      "|    value_loss         | 3.95e+07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1171      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.296    |\n",
      "|    explained_variance | -1.13e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -74.7     |\n",
      "|    value_loss         | 9.2e+05   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1170      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.125    |\n",
      "|    explained_variance | -2.95e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 68.2      |\n",
      "|    value_loss         | 1.2e+08   |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1175     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0966  |\n",
      "|    explained_variance | -8.9e+04 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 85.3     |\n",
      "|    value_loss         | 2.71e+08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1180      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.548    |\n",
      "|    explained_variance | -3.04e+07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -1.93e+03 |\n",
      "|    value_loss         | 1.69e+07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1187      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0721   |\n",
      "|    explained_variance | -1.32e+09 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 276       |\n",
      "|    value_loss         | 5.71e+08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1193      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.209    |\n",
      "|    explained_variance | -7.66e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 8.7e+03   |\n",
      "|    value_loss         | 1.47e+08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1187      |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0328   |\n",
      "|    explained_variance | -3.17e+08 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 59.2      |\n",
      "|    value_loss         | 3.64e+08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1191     |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.358   |\n",
      "|    explained_variance | -7.4e+06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -119     |\n",
      "|    value_loss         | 2.66e+06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1195      |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0176   |\n",
      "|    explained_variance | -4.43e+09 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 52        |\n",
      "|    value_loss         | 5.71e+08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1200      |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0208   |\n",
      "|    explained_variance | -9.87e+08 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 74.6      |\n",
      "|    value_loss         | 1.14e+09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1205      |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0407   |\n",
      "|    explained_variance | -3.28e+09 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | 224       |\n",
      "|    value_loss         | 1.14e+09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1209      |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.271    |\n",
      "|    explained_variance | -5.69e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | -312      |\n",
      "|    value_loss         | 9.9e+07   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1210      |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.111    |\n",
      "|    explained_variance | -9.68e+09 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | 400       |\n",
      "|    value_loss         | 6.62e+08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1214      |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.107    |\n",
      "|    explained_variance | -1.23e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 184       |\n",
      "|    value_loss         | 2.68e+08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1216     |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.488   |\n",
      "|    explained_variance | -3.3e+12 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | -328     |\n",
      "|    value_loss         | 2.65e+06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1219      |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0429   |\n",
      "|    explained_variance | -3.59e+10 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | 1.28e+04  |\n",
      "|    value_loss         | 5.51e+08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1221      |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.218    |\n",
      "|    explained_variance | -5.37e+12 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | -87.5     |\n",
      "|    value_loss         | 2.65e+06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1219      |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.49     |\n",
      "|    explained_variance | -7.99e+06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | -1.44e+03 |\n",
      "|    value_loss         | 1.81e+06  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1221      |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.104    |\n",
      "|    explained_variance | -3.42e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | -1.86e+03 |\n",
      "|    value_loss         | 3.43e+07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1224      |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.468    |\n",
      "|    explained_variance | -4.09e+15 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -2.47e+03 |\n",
      "|    value_loss         | 4.83e+06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1226      |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.203    |\n",
      "|    explained_variance | -1.14e+13 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | -76       |\n",
      "|    value_loss         | 2.65e+06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1228      |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00868  |\n",
      "|    explained_variance | -9.15e+10 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | 24.1      |\n",
      "|    value_loss         | 1.14e+09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1230      |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0513   |\n",
      "|    explained_variance | -2.47e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | 135       |\n",
      "|    value_loss         | 1.49e+08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1232      |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0313   |\n",
      "|    explained_variance | -5.81e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 38.8      |\n",
      "|    value_loss         | 2.65e+08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1234      |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0303   |\n",
      "|    explained_variance | -2.42e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | 66.2      |\n",
      "|    value_loss         | 1.49e+08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1234     |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.121   |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -32      |\n",
      "|    value_loss         | 2.65e+06 |\n",
      "------------------------------------\n",
      "CPU times: user 16.6 s, sys: 318 ms, total: 16.9 s\n",
      "Wall time: 16.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7f7f93bfdca0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "deterministic = False\n",
    "model = A2C(\"MlpPolicy\", env, tensorboard_log=tensorboard_log, verbose=1)\n",
    "model.learn(\n",
    "    total_timesteps=timesteps,\n",
    "    eval_freq=100,\n",
    "    tb_log_name=f\"A2C_train_run_{tb_log_suffix}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward so far: 42000\n",
      "Economic potential: 41000\n",
      "Lost potential: -1000 (-2.439%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Current demand</th>\n",
       "      <th>Next demand</th>\n",
       "      <th>Action</th>\n",
       "      <th>Action description</th>\n",
       "      <th>Reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PREPARE</td>\n",
       "      <td>-2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PREPARE</td>\n",
       "      <td>-2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PREPARE</td>\n",
       "      <td>-2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PREPARE</td>\n",
       "      <td>-2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>60.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>87.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OPERATE</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>56.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>92.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OPERATE</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>83.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OPERATE</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>86.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OPERATE</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>70.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Current demand  Next demand  Action Action description   Reward\n",
       "0              0.0          0.0     3.0              STORE   -500.0\n",
       "1              0.0          0.0     3.0              STORE   -500.0\n",
       "2              0.0          0.0     3.0              STORE   -500.0\n",
       "3              0.0          0.0     3.0              STORE   -500.0\n",
       "4              0.0          0.0     1.0            PREPARE  -2000.0\n",
       "5              0.0          0.0     1.0            PREPARE  -2000.0\n",
       "6              0.0          0.0     1.0            PREPARE  -2000.0\n",
       "7              0.0          0.0     1.0            PREPARE  -2000.0\n",
       "8              0.0          0.0     3.0              STORE   -500.0\n",
       "9              0.0          2.0     3.0              STORE   -500.0\n",
       "10             2.0          0.0     3.0              STORE   -500.0\n",
       "11             0.0          3.0     3.0              STORE   -500.0\n",
       "12             3.0          1.0     3.0              STORE   -500.0\n",
       "13             1.0          0.0     3.0              STORE   -500.0\n",
       "14             0.0          0.0     3.0              STORE   -500.0\n",
       "15             0.0         60.0     3.0              STORE   -500.0\n",
       "16            60.0         25.0     3.0              STORE   -500.0\n",
       "17            25.0         32.0     3.0              STORE   -500.0\n",
       "18            32.0         87.0     3.0              STORE   -500.0\n",
       "19            87.0         56.0     0.0            OPERATE  15000.0\n",
       "20            56.0         92.0     3.0              STORE   -500.0\n",
       "21            92.0         83.0     0.0            OPERATE  15000.0\n",
       "22            83.0         29.0     0.0            OPERATE  15000.0\n",
       "23            29.0         40.0     3.0              STORE   -500.0\n",
       "24            40.0         86.0     3.0              STORE   -500.0\n",
       "25            86.0         70.0     0.0            OPERATE  15000.0\n",
       "26            70.0         45.0     3.0              STORE   -500.0\n",
       "27            45.0          NaN     3.0              STORE   -500.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.fill_table = True\n",
    "obs = env._set_initial_state(initial_state_status=3)\n",
    "while not env.done:\n",
    "    action, _states = model.predict(obs, deterministic=deterministic)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "env.episode_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to logs/rampup_tensorboard/A2C_eval_run_20k_2\n",
      "Eval num_timesteps=100, episode_reward=-11200.00 +/- 8992.22\n",
      "Episode length: 15.00 +/- 5.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200, episode_reward=11100.00 +/- 6865.86\n",
      "Episode length: 18.00 +/- 7.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=300, episode_reward=5800.00 +/- 12643.58\n",
      "Episode length: 13.20 +/- 7.22\n",
      "Eval num_timesteps=400, episode_reward=2100.00 +/- 7499.33\n",
      "Episode length: 11.00 +/- 9.42\n",
      "Eval num_timesteps=500, episode_reward=15400.00 +/- 4200.00\n",
      "Episode length: 12.80 +/- 6.65\n",
      "New best mean reward!\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 12.8      |\n",
      "|    mean_reward        | 1.54e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 814       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.02     |\n",
      "|    explained_variance | -1e+07    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -3.11e+03 |\n",
      "|    value_loss         | 1.83e+07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=600, episode_reward=21600.00 +/- 17881.27\n",
      "Episode length: 11.20 +/- 7.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=700, episode_reward=24400.00 +/- 16122.66\n",
      "Episode length: 20.80 +/- 6.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=800, episode_reward=26200.00 +/- 9982.99\n",
      "Episode length: 17.00 +/- 8.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=900, episode_reward=24100.00 +/- 10960.84\n",
      "Episode length: 16.60 +/- 8.19\n",
      "Eval num_timesteps=1000, episode_reward=22000.00 +/- 7924.65\n",
      "Episode length: 15.80 +/- 8.26\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 15.8      |\n",
      "|    mean_reward        | 2.2e+04   |\n",
      "| time/                 |           |\n",
      "|    fps                | 777       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.773    |\n",
      "|    explained_variance | -3.84e+05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -3.1e+03  |\n",
      "|    value_loss         | 1.69e+07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=1100, episode_reward=18700.00 +/- 12367.70\n",
      "Episode length: 19.60 +/- 7.61\n",
      "Eval num_timesteps=1200, episode_reward=32400.00 +/- 12827.31\n",
      "Episode length: 11.20 +/- 6.18\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1300, episode_reward=39300.00 +/- 6720.12\n",
      "Episode length: 17.60 +/- 4.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1400, episode_reward=39800.00 +/- 2481.93\n",
      "Episode length: 15.00 +/- 6.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=39300.00 +/- 10879.34\n",
      "Episode length: 13.60 +/- 4.67\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 13.6      |\n",
      "|    mean_reward        | 3.93e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 781       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.17     |\n",
      "|    explained_variance | -4.59e+07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -1.17e+04 |\n",
      "|    value_loss         | 7.34e+07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=1600, episode_reward=34500.00 +/- 10416.33\n",
      "Episode length: 19.20 +/- 6.62\n",
      "Eval num_timesteps=1700, episode_reward=38800.00 +/- 9667.47\n",
      "Episode length: 13.60 +/- 5.95\n",
      "Eval num_timesteps=1800, episode_reward=41400.00 +/- 6248.20\n",
      "Episode length: 18.20 +/- 5.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1900, episode_reward=33600.00 +/- 19558.63\n",
      "Episode length: 11.80 +/- 6.62\n",
      "Eval num_timesteps=2000, episode_reward=33000.00 +/- 14923.14\n",
      "Episode length: 15.00 +/- 5.22\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 15        |\n",
      "|    mean_reward        | 3.3e+04   |\n",
      "| time/                 |           |\n",
      "|    fps                | 800       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.12     |\n",
      "|    explained_variance | -3.57e+07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -5.45e+03 |\n",
      "|    value_loss         | 2.43e+07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=2100, episode_reward=39200.00 +/- 4081.67\n",
      "Episode length: 20.00 +/- 4.00\n",
      "Eval num_timesteps=2200, episode_reward=36900.00 +/- 16992.35\n",
      "Episode length: 13.00 +/- 6.42\n",
      "Eval num_timesteps=2300, episode_reward=30800.00 +/- 11565.47\n",
      "Episode length: 16.60 +/- 8.80\n",
      "Eval num_timesteps=2400, episode_reward=26100.00 +/- 19642.81\n",
      "Episode length: 14.00 +/- 8.37\n",
      "Eval num_timesteps=2500, episode_reward=36800.00 +/- 14197.18\n",
      "Episode length: 8.00 +/- 3.58\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 8         |\n",
      "|    mean_reward        | 3.68e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 800       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.06     |\n",
      "|    explained_variance | -1.45e+07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -1.93e+03 |\n",
      "|    value_loss         | 6.28e+06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=2600, episode_reward=29000.00 +/- 14869.43\n",
      "Episode length: 14.20 +/- 5.42\n",
      "Eval num_timesteps=2700, episode_reward=27600.00 +/- 15183.54\n",
      "Episode length: 14.00 +/- 8.90\n",
      "Eval num_timesteps=2800, episode_reward=41600.00 +/- 5416.64\n",
      "Episode length: 20.40 +/- 5.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2900, episode_reward=39300.00 +/- 14732.96\n",
      "Episode length: 12.00 +/- 5.76\n",
      "Eval num_timesteps=3000, episode_reward=36800.00 +/- 15354.48\n",
      "Episode length: 14.20 +/- 6.97\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 14.2      |\n",
      "|    mean_reward        | 3.68e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 807       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.433    |\n",
      "|    explained_variance | -1.34e+07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 915       |\n",
      "|    value_loss         | 2.12e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=3100, episode_reward=39500.00 +/- 12312.59\n",
      "Episode length: 21.80 +/- 6.31\n",
      "Eval num_timesteps=3200, episode_reward=46500.00 +/- 6804.41\n",
      "Episode length: 18.40 +/- 6.12\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3300, episode_reward=31900.00 +/- 18139.46\n",
      "Episode length: 13.60 +/- 7.76\n",
      "Eval num_timesteps=3400, episode_reward=42200.00 +/- 6600.00\n",
      "Episode length: 16.00 +/- 6.36\n",
      "Eval num_timesteps=3500, episode_reward=45100.00 +/- 3813.14\n",
      "Episode length: 14.40 +/- 4.03\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 14.4      |\n",
      "|    mean_reward        | 4.51e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 799       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.682    |\n",
      "|    explained_variance | -2.18e+05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -3.1e+03  |\n",
      "|    value_loss         | 3.84e+07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=3600, episode_reward=46300.00 +/- 6337.19\n",
      "Episode length: 14.80 +/- 3.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=3700, episode_reward=43000.00 +/- 5567.76\n",
      "Episode length: 18.20 +/- 6.46\n",
      "Eval num_timesteps=3800, episode_reward=31800.00 +/- 13470.71\n",
      "Episode length: 12.00 +/- 7.62\n",
      "Eval num_timesteps=3900, episode_reward=42900.00 +/- 15464.15\n",
      "Episode length: 13.00 +/- 6.69\n",
      "Eval num_timesteps=4000, episode_reward=34000.00 +/- 18936.74\n",
      "Episode length: 17.20 +/- 7.83\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 17.2      |\n",
      "|    mean_reward        | 3.4e+04   |\n",
      "| time/                 |           |\n",
      "|    fps                | 792       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.241    |\n",
      "|    explained_variance | -2.94e+08 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 1.83e+03  |\n",
      "|    value_loss         | 1.14e+09  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=4100, episode_reward=28000.00 +/- 19429.36\n",
      "Episode length: 10.40 +/- 7.23\n",
      "Eval num_timesteps=4200, episode_reward=37400.00 +/- 14884.22\n",
      "Episode length: 14.60 +/- 8.01\n",
      "Eval num_timesteps=4300, episode_reward=45900.00 +/- 3839.27\n",
      "Episode length: 17.20 +/- 2.56\n",
      "Eval num_timesteps=4400, episode_reward=42100.00 +/- 16457.22\n",
      "Episode length: 13.00 +/- 7.10\n",
      "Eval num_timesteps=4500, episode_reward=39100.00 +/- 22030.43\n",
      "Episode length: 10.00 +/- 5.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 10       |\n",
      "|    mean_reward        | 3.91e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 789      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.74    |\n",
      "|    explained_variance | -7.2e+04 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -346     |\n",
      "|    value_loss         | 5.28e+07 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4600, episode_reward=20800.00 +/- 28173.75\n",
      "Episode length: 5.40 +/- 4.27\n",
      "Eval num_timesteps=4700, episode_reward=41800.00 +/- 16357.26\n",
      "Episode length: 15.00 +/- 7.35\n",
      "Eval num_timesteps=4800, episode_reward=40900.00 +/- 8817.03\n",
      "Episode length: 17.40 +/- 8.21\n",
      "Eval num_timesteps=4900, episode_reward=46500.00 +/- 2966.48\n",
      "Episode length: 17.20 +/- 5.00\n",
      "Eval num_timesteps=5000, episode_reward=39800.00 +/- 509.90\n",
      "Episode length: 12.20 +/- 6.05\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 12.2      |\n",
      "|    mean_reward        | 3.98e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 788       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.396    |\n",
      "|    explained_variance | -1.27e+06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 802       |\n",
      "|    value_loss         | 2.73e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=5100, episode_reward=35300.00 +/- 14009.28\n",
      "Episode length: 16.20 +/- 7.30\n",
      "Eval num_timesteps=5200, episode_reward=23900.00 +/- 18861.07\n",
      "Episode length: 9.80 +/- 7.36\n",
      "Eval num_timesteps=5300, episode_reward=41600.00 +/- 8714.36\n",
      "Episode length: 14.80 +/- 5.08\n",
      "Eval num_timesteps=5400, episode_reward=20600.00 +/- 20222.26\n",
      "Episode length: 8.60 +/- 7.58\n",
      "Eval num_timesteps=5500, episode_reward=39700.00 +/- 4760.25\n",
      "Episode length: 18.20 +/- 5.67\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 18.2      |\n",
      "|    mean_reward        | 3.97e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 792       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.298    |\n",
      "|    explained_variance | -4.33e+08 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 6.78e+03  |\n",
      "|    value_loss         | 5.69e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=5600, episode_reward=32900.00 +/- 12634.87\n",
      "Episode length: 18.40 +/- 7.14\n",
      "Eval num_timesteps=5700, episode_reward=47200.00 +/- 6104.10\n",
      "Episode length: 11.20 +/- 3.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5800, episode_reward=26900.00 +/- 21615.73\n",
      "Episode length: 13.20 +/- 9.20\n",
      "Eval num_timesteps=5900, episode_reward=25800.00 +/- 13071.34\n",
      "Episode length: 11.80 +/- 9.28\n",
      "Eval num_timesteps=6000, episode_reward=31900.00 +/- 12495.60\n",
      "Episode length: 16.00 +/- 9.36\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 16        |\n",
      "|    mean_reward        | 3.19e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 796       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.499    |\n",
      "|    explained_variance | -2.65e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 1.08e+03  |\n",
      "|    value_loss         | 1.46e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=6100, episode_reward=41400.00 +/- 15379.86\n",
      "Episode length: 10.80 +/- 3.92\n",
      "Eval num_timesteps=6200, episode_reward=37700.00 +/- 15451.86\n",
      "Episode length: 14.80 +/- 7.47\n",
      "Eval num_timesteps=6300, episode_reward=47900.00 +/- 3583.29\n",
      "Episode length: 14.60 +/- 3.44\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6400, episode_reward=33800.00 +/- 14105.32\n",
      "Episode length: 12.80 +/- 8.03\n",
      "Eval num_timesteps=6500, episode_reward=37500.00 +/- 14099.64\n",
      "Episode length: 14.80 +/- 8.23\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 14.8      |\n",
      "|    mean_reward        | 3.75e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 799       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.495    |\n",
      "|    explained_variance | -3.04e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 1.61e+03  |\n",
      "|    value_loss         | 1.36e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=6600, episode_reward=48500.00 +/- 4868.26\n",
      "Episode length: 16.40 +/- 5.64\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6700, episode_reward=47000.00 +/- 4472.14\n",
      "Episode length: 16.60 +/- 5.08\n",
      "Eval num_timesteps=6800, episode_reward=40700.00 +/- 5564.17\n",
      "Episode length: 22.40 +/- 4.08\n",
      "Eval num_timesteps=6900, episode_reward=36500.00 +/- 13479.61\n",
      "Episode length: 15.80 +/- 7.44\n",
      "Eval num_timesteps=7000, episode_reward=21500.00 +/- 20415.68\n",
      "Episode length: 8.60 +/- 7.58\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 8.6       |\n",
      "|    mean_reward        | 2.15e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 802       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.462    |\n",
      "|    explained_variance | -3.19e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 1.23e+03  |\n",
      "|    value_loss         | 7.37e+07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=7100, episode_reward=46800.00 +/- 6705.22\n",
      "Episode length: 14.40 +/- 5.50\n",
      "Eval num_timesteps=7200, episode_reward=28900.00 +/- 15496.45\n",
      "Episode length: 12.40 +/- 6.89\n",
      "Eval num_timesteps=7300, episode_reward=45900.00 +/- 2690.72\n",
      "Episode length: 20.00 +/- 4.94\n",
      "Eval num_timesteps=7400, episode_reward=40400.00 +/- 13324.41\n",
      "Episode length: 15.20 +/- 6.37\n",
      "Eval num_timesteps=7500, episode_reward=40500.00 +/- 15122.83\n",
      "Episode length: 14.20 +/- 7.41\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 14.2      |\n",
      "|    mean_reward        | 4.05e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 806       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.689    |\n",
      "|    explained_variance | -5.88e+10 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -1.09e+03 |\n",
      "|    value_loss         | 3.2e+06   |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=7600, episode_reward=48600.00 +/- 4789.57\n",
      "Episode length: 13.60 +/- 4.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7700, episode_reward=31300.00 +/- 17721.74\n",
      "Episode length: 15.20 +/- 8.40\n",
      "Eval num_timesteps=7800, episode_reward=46700.00 +/- 3370.46\n",
      "Episode length: 20.40 +/- 2.42\n",
      "Eval num_timesteps=7900, episode_reward=40400.00 +/- 15435.02\n",
      "Episode length: 14.00 +/- 5.48\n",
      "Eval num_timesteps=8000, episode_reward=28300.00 +/- 16039.33\n",
      "Episode length: 13.60 +/- 9.33\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 13.6      |\n",
      "|    mean_reward        | 2.83e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 800       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.477    |\n",
      "|    explained_variance | -2.36e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -1.13e+03 |\n",
      "|    value_loss         | 5.18e+07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=8100, episode_reward=33600.00 +/- 14147.08\n",
      "Episode length: 16.00 +/- 8.85\n",
      "Eval num_timesteps=8200, episode_reward=44500.00 +/- 2756.81\n",
      "Episode length: 21.20 +/- 4.35\n",
      "Eval num_timesteps=8300, episode_reward=35500.00 +/- 20029.98\n",
      "Episode length: 10.20 +/- 5.46\n",
      "Eval num_timesteps=8400, episode_reward=34300.00 +/- 19033.13\n",
      "Episode length: 15.60 +/- 9.44\n",
      "Eval num_timesteps=8500, episode_reward=37300.00 +/- 19600.00\n",
      "Episode length: 9.60 +/- 5.31\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 9.6       |\n",
      "|    mean_reward        | 3.73e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 795       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.158    |\n",
      "|    explained_variance | -1.54e+08 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 667       |\n",
      "|    value_loss         | 6.62e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=8600, episode_reward=45800.00 +/- 4523.27\n",
      "Episode length: 14.80 +/- 5.42\n",
      "Eval num_timesteps=8700, episode_reward=48100.00 +/- 7552.48\n",
      "Episode length: 19.00 +/- 6.42\n",
      "Eval num_timesteps=8800, episode_reward=51600.00 +/- 5219.20\n",
      "Episode length: 11.20 +/- 4.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8900, episode_reward=51600.00 +/- 2517.94\n",
      "Episode length: 17.80 +/- 4.92\n",
      "Eval num_timesteps=9000, episode_reward=46000.00 +/- 3178.05\n",
      "Episode length: 16.20 +/- 7.05\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 16.2      |\n",
      "|    mean_reward        | 4.6e+04   |\n",
      "| time/                 |           |\n",
      "|    fps                | 798       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.305    |\n",
      "|    explained_variance | -7.18e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 668       |\n",
      "|    value_loss         | 2.71e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=9100, episode_reward=41200.00 +/- 21542.05\n",
      "Episode length: 15.40 +/- 8.66\n",
      "Eval num_timesteps=9200, episode_reward=39600.00 +/- 14961.28\n",
      "Episode length: 10.40 +/- 5.35\n",
      "Eval num_timesteps=9300, episode_reward=34800.00 +/- 13962.81\n",
      "Episode length: 12.00 +/- 7.56\n",
      "Eval num_timesteps=9400, episode_reward=42800.00 +/- 8494.70\n",
      "Episode length: 17.40 +/- 7.45\n",
      "Eval num_timesteps=9500, episode_reward=46500.00 +/- 5263.08\n",
      "Episode length: 13.20 +/- 5.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 13.2     |\n",
      "|    mean_reward        | 4.65e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 803      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.235   |\n",
      "|    explained_variance | -196     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 829      |\n",
      "|    value_loss         | 1.9e+08  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9600, episode_reward=45800.00 +/- 16657.13\n",
      "Episode length: 12.80 +/- 6.24\n",
      "Eval num_timesteps=9700, episode_reward=50400.00 +/- 2634.39\n",
      "Episode length: 18.60 +/- 5.89\n",
      "Eval num_timesteps=9800, episode_reward=50700.00 +/- 2731.30\n",
      "Episode length: 19.20 +/- 5.27\n",
      "Eval num_timesteps=9900, episode_reward=39700.00 +/- 21139.06\n",
      "Episode length: 18.20 +/- 8.52\n",
      "Eval num_timesteps=10000, episode_reward=47500.00 +/- 6418.72\n",
      "Episode length: 13.00 +/- 2.97\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 13        |\n",
      "|    mean_reward        | 4.75e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 792       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.397    |\n",
      "|    explained_variance | -1.32e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -49       |\n",
      "|    value_loss         | 7.46e+07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=10100, episode_reward=42300.00 +/- 14596.58\n",
      "Episode length: 16.20 +/- 7.47\n",
      "Eval num_timesteps=10200, episode_reward=40400.00 +/- 10389.42\n",
      "Episode length: 13.80 +/- 5.11\n",
      "Eval num_timesteps=10300, episode_reward=29400.00 +/- 16227.75\n",
      "Episode length: 8.00 +/- 4.47\n",
      "Eval num_timesteps=10400, episode_reward=47100.00 +/- 3693.24\n",
      "Episode length: 20.40 +/- 4.32\n",
      "Eval num_timesteps=10500, episode_reward=39600.00 +/- 22125.55\n",
      "Episode length: 12.00 +/- 7.46\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 12        |\n",
      "|    mean_reward        | 3.96e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 793       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.561    |\n",
      "|    explained_variance | -1.01e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -11.1     |\n",
      "|    value_loss         | 6.8e+07   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=10600, episode_reward=40800.00 +/- 15759.44\n",
      "Episode length: 15.20 +/- 8.82\n",
      "Eval num_timesteps=10700, episode_reward=47200.00 +/- 3957.27\n",
      "Episode length: 14.20 +/- 4.87\n",
      "Eval num_timesteps=10800, episode_reward=38300.00 +/- 21303.99\n",
      "Episode length: 16.20 +/- 8.68\n",
      "Eval num_timesteps=10900, episode_reward=31100.00 +/- 23373.92\n",
      "Episode length: 6.40 +/- 3.26\n",
      "Eval num_timesteps=11000, episode_reward=46600.00 +/- 4188.08\n",
      "Episode length: 17.40 +/- 6.95\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 17.4      |\n",
      "|    mean_reward        | 4.66e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 796       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.447    |\n",
      "|    explained_variance | -7.61e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 6.21e+03  |\n",
      "|    value_loss         | 4.14e+07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=11100, episode_reward=42600.00 +/- 15960.58\n",
      "Episode length: 13.00 +/- 7.46\n",
      "Eval num_timesteps=11200, episode_reward=41100.00 +/- 14619.85\n",
      "Episode length: 14.40 +/- 7.39\n",
      "Eval num_timesteps=11300, episode_reward=31300.00 +/- 17820.21\n",
      "Episode length: 6.00 +/- 2.19\n",
      "Eval num_timesteps=11400, episode_reward=42800.00 +/- 16228.37\n",
      "Episode length: 14.00 +/- 5.83\n",
      "Eval num_timesteps=11500, episode_reward=38100.00 +/- 20874.39\n",
      "Episode length: 16.40 +/- 7.91\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 16.4      |\n",
      "|    mean_reward        | 3.81e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 795       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.379    |\n",
      "|    explained_variance | -3.61e+08 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -6.49e+03 |\n",
      "|    value_loss         | 2.5e+07   |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=11600, episode_reward=42800.00 +/- 7613.15\n",
      "Episode length: 19.20 +/- 6.71\n",
      "Eval num_timesteps=11700, episode_reward=38400.00 +/- 21214.15\n",
      "Episode length: 16.80 +/- 8.93\n",
      "Eval num_timesteps=11800, episode_reward=48500.00 +/- 3193.74\n",
      "Episode length: 22.60 +/- 6.47\n",
      "Eval num_timesteps=11900, episode_reward=42100.00 +/- 15954.31\n",
      "Episode length: 15.80 +/- 7.19\n",
      "Eval num_timesteps=12000, episode_reward=45600.00 +/- 3760.32\n",
      "Episode length: 16.60 +/- 6.59\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 16.6      |\n",
      "|    mean_reward        | 4.56e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 794       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.422    |\n",
      "|    explained_variance | -2.5e+13  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | -5.72e+03 |\n",
      "|    value_loss         | 9.02e+06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=12100, episode_reward=48300.00 +/- 5287.72\n",
      "Episode length: 18.80 +/- 5.95\n",
      "Eval num_timesteps=12200, episode_reward=50600.00 +/- 2956.35\n",
      "Episode length: 19.00 +/- 4.60\n",
      "Eval num_timesteps=12300, episode_reward=43600.00 +/- 16841.62\n",
      "Episode length: 14.60 +/- 7.31\n",
      "Eval num_timesteps=12400, episode_reward=52800.00 +/- 6469.93\n",
      "Episode length: 9.40 +/- 1.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12500, episode_reward=38800.00 +/- 15164.43\n",
      "Episode length: 15.80 +/- 8.45\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 15.8      |\n",
      "|    mean_reward        | 3.88e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 794       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.583    |\n",
      "|    explained_variance | -6.4e+03  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | -3.78e+03 |\n",
      "|    value_loss         | 2.81e+07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=12600, episode_reward=42400.00 +/- 17159.25\n",
      "Episode length: 16.60 +/- 7.76\n",
      "Eval num_timesteps=12700, episode_reward=44000.00 +/- 15178.93\n",
      "Episode length: 12.60 +/- 5.08\n",
      "Eval num_timesteps=12800, episode_reward=43700.00 +/- 16818.44\n",
      "Episode length: 15.40 +/- 6.47\n",
      "Eval num_timesteps=12900, episode_reward=24900.00 +/- 21975.90\n",
      "Episode length: 11.20 +/- 9.28\n",
      "Eval num_timesteps=13000, episode_reward=46800.00 +/- 6217.72\n",
      "Episode length: 14.20 +/- 6.97\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 14.2      |\n",
      "|    mean_reward        | 4.68e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 798       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0611   |\n",
      "|    explained_variance | -8.68e+08 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | 226       |\n",
      "|    value_loss         | 5.71e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=13100, episode_reward=45600.00 +/- 4933.56\n",
      "Episode length: 16.60 +/- 6.53\n",
      "Eval num_timesteps=13200, episode_reward=36800.00 +/- 14158.39\n",
      "Episode length: 14.80 +/- 8.52\n",
      "Eval num_timesteps=13300, episode_reward=39600.00 +/- 21870.98\n",
      "Episode length: 14.00 +/- 6.60\n",
      "Eval num_timesteps=13400, episode_reward=26100.00 +/- 19958.46\n",
      "Episode length: 9.80 +/- 7.55\n",
      "Eval num_timesteps=13500, episode_reward=30000.00 +/- 15323.18\n",
      "Episode length: 11.60 +/- 8.75\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 11.6      |\n",
      "|    mean_reward        | 3e+04     |\n",
      "| time/                 |           |\n",
      "|    fps                | 801       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.336    |\n",
      "|    explained_variance | -6.09e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | -23.6     |\n",
      "|    value_loss         | 1.06e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=13600, episode_reward=48400.00 +/- 5517.25\n",
      "Episode length: 13.40 +/- 5.39\n",
      "Eval num_timesteps=13700, episode_reward=41900.00 +/- 22937.74\n",
      "Episode length: 11.40 +/- 7.50\n",
      "Eval num_timesteps=13800, episode_reward=30100.00 +/- 19837.84\n",
      "Episode length: 10.20 +/- 7.00\n",
      "Eval num_timesteps=13900, episode_reward=41000.00 +/- 9402.13\n",
      "Episode length: 17.40 +/- 6.74\n",
      "Eval num_timesteps=14000, episode_reward=46700.00 +/- 3586.08\n",
      "Episode length: 18.40 +/- 5.89\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 18.4      |\n",
      "|    mean_reward        | 4.67e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 804       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.042    |\n",
      "|    explained_variance | -6.45e+09 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 105       |\n",
      "|    value_loss         | 6.62e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=14100, episode_reward=42500.00 +/- 16087.26\n",
      "Episode length: 14.40 +/- 7.23\n",
      "Eval num_timesteps=14200, episode_reward=50700.00 +/- 2158.70\n",
      "Episode length: 17.40 +/- 4.50\n",
      "Eval num_timesteps=14300, episode_reward=51800.00 +/- 1600.00\n",
      "Episode length: 15.20 +/- 3.19\n",
      "Eval num_timesteps=14400, episode_reward=37800.00 +/- 17898.60\n",
      "Episode length: 11.40 +/- 6.31\n",
      "Eval num_timesteps=14500, episode_reward=48600.00 +/- 4619.52\n",
      "Episode length: 14.60 +/- 4.80\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 14.6      |\n",
      "|    mean_reward        | 4.86e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 806       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0701   |\n",
      "|    explained_variance | -1.33e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | 66.7      |\n",
      "|    value_loss         | 2.07e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=14600, episode_reward=42400.00 +/- 15528.68\n",
      "Episode length: 15.60 +/- 7.03\n",
      "Eval num_timesteps=14700, episode_reward=38600.00 +/- 20266.72\n",
      "Episode length: 11.80 +/- 6.68\n",
      "Eval num_timesteps=14800, episode_reward=45600.00 +/- 16332.18\n",
      "Episode length: 13.00 +/- 4.47\n",
      "Eval num_timesteps=14900, episode_reward=41100.00 +/- 15412.33\n",
      "Episode length: 13.80 +/- 8.08\n",
      "Eval num_timesteps=15000, episode_reward=53600.00 +/- 1356.47\n",
      "Episode length: 13.60 +/- 2.94\n",
      "New best mean reward!\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 13.6      |\n",
      "|    mean_reward        | 5.36e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 808       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0304   |\n",
      "|    explained_variance | -8.21e+09 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | 126       |\n",
      "|    value_loss         | 1.14e+09  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=15100, episode_reward=43000.00 +/- 10373.04\n",
      "Episode length: 10.20 +/- 4.58\n",
      "Eval num_timesteps=15200, episode_reward=43600.00 +/- 8633.66\n",
      "Episode length: 16.20 +/- 6.76\n",
      "Eval num_timesteps=15300, episode_reward=42400.00 +/- 21820.63\n",
      "Episode length: 12.40 +/- 6.56\n",
      "Eval num_timesteps=15400, episode_reward=32400.00 +/- 22053.12\n",
      "Episode length: 13.20 +/- 8.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=15500, episode_reward=40500.00 +/- 12601.59\n",
      "Episode length: 9.60 +/- 4.59\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 9.6       |\n",
      "|    mean_reward        | 4.05e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 809       |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.203    |\n",
      "|    explained_variance | -3.55e+05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | -63.4     |\n",
      "|    value_loss         | 8.75e+05  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=15600, episode_reward=44900.00 +/- 16128.86\n",
      "Episode length: 13.60 +/- 5.24\n",
      "Eval num_timesteps=15700, episode_reward=43300.00 +/- 15038.62\n",
      "Episode length: 16.00 +/- 7.90\n",
      "Eval num_timesteps=15800, episode_reward=47800.00 +/- 11483.03\n",
      "Episode length: 14.40 +/- 6.12\n",
      "Eval num_timesteps=15900, episode_reward=32400.00 +/- 16974.69\n",
      "Episode length: 9.40 +/- 7.61\n",
      "Eval num_timesteps=16000, episode_reward=40400.00 +/- 17121.33\n",
      "Episode length: 12.20 +/- 7.52\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 12.2      |\n",
      "|    mean_reward        | 4.04e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 811       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0308   |\n",
      "|    explained_variance | -8.12e+10 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 97        |\n",
      "|    value_loss         | 6.62e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=16100, episode_reward=34300.00 +/- 23578.80\n",
      "Episode length: 11.80 +/- 7.44\n",
      "Eval num_timesteps=16200, episode_reward=51700.00 +/- 4843.55\n",
      "Episode length: 13.80 +/- 5.04\n",
      "Eval num_timesteps=16300, episode_reward=42200.00 +/- 14651.28\n",
      "Episode length: 14.80 +/- 8.66\n",
      "Eval num_timesteps=16400, episode_reward=49700.00 +/- 4118.25\n",
      "Episode length: 17.20 +/- 6.11\n",
      "Eval num_timesteps=16500, episode_reward=28100.00 +/- 25941.09\n",
      "Episode length: 13.00 +/- 10.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 13       |\n",
      "|    mean_reward        | 2.81e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 811      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0846  |\n",
      "|    explained_variance | -45.2    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | -26.2    |\n",
      "|    value_loss         | 2.08e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=16600, episode_reward=45300.00 +/- 10689.25\n",
      "Episode length: 11.80 +/- 5.27\n",
      "Eval num_timesteps=16700, episode_reward=41000.00 +/- 14720.73\n",
      "Episode length: 8.80 +/- 3.66\n",
      "Eval num_timesteps=16800, episode_reward=39000.00 +/- 15971.85\n",
      "Episode length: 14.80 +/- 7.83\n",
      "Eval num_timesteps=16900, episode_reward=54600.00 +/- 2745.91\n",
      "Episode length: 14.80 +/- 5.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=43700.00 +/- 22408.93\n",
      "Episode length: 10.80 +/- 4.71\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 10.8      |\n",
      "|    mean_reward        | 4.37e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 811       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0324   |\n",
      "|    explained_variance | -7.62e+10 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 111       |\n",
      "|    value_loss         | 6.62e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=17100, episode_reward=45000.00 +/- 15801.90\n",
      "Episode length: 15.40 +/- 6.25\n",
      "Eval num_timesteps=17200, episode_reward=26800.00 +/- 17078.05\n",
      "Episode length: 8.40 +/- 5.08\n",
      "Eval num_timesteps=17300, episode_reward=53200.00 +/- 3234.19\n",
      "Episode length: 17.40 +/- 6.18\n",
      "Eval num_timesteps=17400, episode_reward=49400.00 +/- 6019.97\n",
      "Episode length: 18.20 +/- 4.31\n",
      "Eval num_timesteps=17500, episode_reward=42200.00 +/- 15058.55\n",
      "Episode length: 18.80 +/- 9.04\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 18.8      |\n",
      "|    mean_reward        | 4.22e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 809       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0447   |\n",
      "|    explained_variance | -3.64e+10 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | 83.6      |\n",
      "|    value_loss         | 3.64e+08  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=17600, episode_reward=49100.00 +/- 7102.11\n",
      "Episode length: 19.40 +/- 2.24\n",
      "Eval num_timesteps=17700, episode_reward=44700.00 +/- 15644.81\n",
      "Episode length: 14.20 +/- 6.79\n",
      "Eval num_timesteps=17800, episode_reward=49900.00 +/- 4619.52\n",
      "Episode length: 18.00 +/- 8.25\n",
      "Eval num_timesteps=17900, episode_reward=51700.00 +/- 4781.21\n",
      "Episode length: 14.00 +/- 4.82\n",
      "Eval num_timesteps=18000, episode_reward=35500.00 +/- 24168.16\n",
      "Episode length: 9.60 +/- 6.12\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 9.6       |\n",
      "|    mean_reward        | 3.55e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 807       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.132    |\n",
      "|    explained_variance | -3.16e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -20.5     |\n",
      "|    value_loss         | 3.9e+07   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=18100, episode_reward=51100.00 +/- 4933.56\n",
      "Episode length: 14.80 +/- 5.11\n",
      "Eval num_timesteps=18200, episode_reward=47200.00 +/- 6860.03\n",
      "Episode length: 16.60 +/- 4.63\n",
      "Eval num_timesteps=18300, episode_reward=55200.00 +/- 2039.61\n",
      "Episode length: 13.60 +/- 4.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18400, episode_reward=32800.00 +/- 22213.96\n",
      "Episode length: 12.00 +/- 8.02\n",
      "Eval num_timesteps=18500, episode_reward=43800.00 +/- 15622.42\n",
      "Episode length: 17.40 +/- 7.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 17.4     |\n",
      "|    mean_reward        | 4.38e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 804      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.279   |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | -665     |\n",
      "|    value_loss         | 4.33e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=18600, episode_reward=44900.00 +/- 17442.48\n",
      "Episode length: 13.20 +/- 7.08\n",
      "Eval num_timesteps=18700, episode_reward=47400.00 +/- 10076.71\n",
      "Episode length: 16.60 +/- 7.42\n",
      "Eval num_timesteps=18800, episode_reward=35700.00 +/- 20380.38\n",
      "Episode length: 14.40 +/- 9.05\n",
      "Eval num_timesteps=18900, episode_reward=43100.00 +/- 15698.41\n",
      "Episode length: 12.40 +/- 5.00\n",
      "Eval num_timesteps=19000, episode_reward=51100.00 +/- 7179.14\n",
      "Episode length: 15.40 +/- 3.20\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 15.4      |\n",
      "|    mean_reward        | 5.11e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 805       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0222   |\n",
      "|    explained_variance | -1.76e+10 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 1.42e+04  |\n",
      "|    value_loss         | 3.14e+08  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=19100, episode_reward=40000.00 +/- 17073.37\n",
      "Episode length: 12.80 +/- 7.55\n",
      "Eval num_timesteps=19200, episode_reward=46600.00 +/- 3733.63\n",
      "Episode length: 21.00 +/- 7.07\n",
      "Eval num_timesteps=19300, episode_reward=50400.00 +/- 4079.22\n",
      "Episode length: 17.00 +/- 6.07\n",
      "Eval num_timesteps=19400, episode_reward=49800.00 +/- 3854.87\n",
      "Episode length: 18.20 +/- 6.68\n",
      "Eval num_timesteps=19500, episode_reward=55700.00 +/- 2014.94\n",
      "Episode length: 12.60 +/- 4.03\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 12.6     |\n",
      "|    mean_reward        | 5.57e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 806      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.278   |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | -110     |\n",
      "|    value_loss         | 2.65e+06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=19600, episode_reward=40700.00 +/- 13844.13\n",
      "Episode length: 17.20 +/- 9.68\n",
      "Eval num_timesteps=19700, episode_reward=51200.00 +/- 2908.61\n",
      "Episode length: 18.80 +/- 6.05\n",
      "Eval num_timesteps=19800, episode_reward=40700.00 +/- 21364.92\n",
      "Episode length: 11.60 +/- 6.34\n",
      "Eval num_timesteps=19900, episode_reward=49500.00 +/- 1414.21\n",
      "Episode length: 23.40 +/- 2.24\n",
      "Eval num_timesteps=20000, episode_reward=40200.00 +/- 16029.97\n",
      "Episode length: 15.00 +/- 7.29\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 15        |\n",
      "|    mean_reward        | 4.02e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 807       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.299    |\n",
      "|    explained_variance | -1.95e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -122      |\n",
      "|    value_loss         | 2.85e+07  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7f7f971f1d60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate evaluation env\n",
    "# eval_env = RampupEnv1(demand)\n",
    "eval_env = env\n",
    "# Use deterministic actions for evaluation (that seems like #bs)\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./logs/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=100,\n",
    "    deterministic=deterministic,\n",
    "    render=False,\n",
    ")\n",
    "\n",
    "eval_model = A2C(\"MlpPolicy\", eval_env, tensorboard_log=tensorboard_log, verbose=1)\n",
    "eval_model.learn(\n",
    "    total_timesteps=timesteps,\n",
    "    callback=eval_callback,\n",
    "    tb_log_name=f\"A2C_eval_run_{tb_log_suffix}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward so far: 43000\n",
      "Economic potential: 41000\n",
      "Lost potential: -2000 (-4.878%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Current demand</th>\n",
       "      <th>Next demand</th>\n",
       "      <th>Action</th>\n",
       "      <th>Action description</th>\n",
       "      <th>Reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>60.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>87.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OPERATE</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>56.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>92.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OPERATE</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>83.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OPERATE</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>STORE</td>\n",
       "      <td>-500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>86.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OPERATE</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>70.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OPERATE</td>\n",
       "      <td>-3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OPERATE</td>\n",
       "      <td>-3000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Current demand  Next demand  Action Action description   Reward\n",
       "0              0.0          0.0     3.0              STORE   -500.0\n",
       "1              0.0          0.0     3.0              STORE   -500.0\n",
       "2              0.0          0.0     3.0              STORE   -500.0\n",
       "3              0.0          0.0     3.0              STORE   -500.0\n",
       "4              0.0          0.0     3.0              STORE   -500.0\n",
       "5              0.0          0.0     3.0              STORE   -500.0\n",
       "6              0.0          0.0     3.0              STORE   -500.0\n",
       "7              0.0          0.0     3.0              STORE   -500.0\n",
       "8              0.0          0.0     3.0              STORE   -500.0\n",
       "9              0.0          2.0     3.0              STORE   -500.0\n",
       "10             2.0          0.0     3.0              STORE   -500.0\n",
       "11             0.0          3.0     3.0              STORE   -500.0\n",
       "12             3.0          1.0     3.0              STORE   -500.0\n",
       "13             1.0          0.0     3.0              STORE   -500.0\n",
       "14             0.0          0.0     3.0              STORE   -500.0\n",
       "15             0.0         60.0     3.0              STORE   -500.0\n",
       "16            60.0         25.0     3.0              STORE   -500.0\n",
       "17            25.0         32.0     3.0              STORE   -500.0\n",
       "18            32.0         87.0     3.0              STORE   -500.0\n",
       "19            87.0         56.0     0.0            OPERATE  15000.0\n",
       "20            56.0         92.0     3.0              STORE   -500.0\n",
       "21            92.0         83.0     0.0            OPERATE  15000.0\n",
       "22            83.0         29.0     0.0            OPERATE  15000.0\n",
       "23            29.0         40.0     3.0              STORE   -500.0\n",
       "24            40.0         86.0     3.0              STORE   -500.0\n",
       "25            86.0         70.0     0.0            OPERATE  15000.0\n",
       "26            70.0         45.0     0.0            OPERATE  -3000.0\n",
       "27            45.0          NaN     0.0            OPERATE  -3000.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env.fill_table = True\n",
    "obs = eval_env._set_initial_state(initial_state_status=3)\n",
    "while not eval_env.done:\n",
    "    action, _states = eval_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = eval_env.step(action)\n",
    "eval_env.render()\n",
    "eval_env.episode_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluated for 10 episodes with a mean reward of 44700 and a standard deviation of 8803.\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(eval_model, eval_env)\n",
    "print(\n",
    "    f\"Policy evaluated for 10 episodes with a mean reward of {int(mean_reward)} and a standard deviation of {int(std_reward)}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "Start Tensorboard on port 6006 and open it in a browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 0:\n",
    "    pid = subprocess.Popen(\n",
    "        [\"tensorboard\", \"--logdir\", f\"./{tensorboard_log}\", \"--port\", \"6006\"]\n",
    "    )\n",
    "    os.system(\"sleep 5\")\n",
    "    webbrowser.open(\"http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, load the TensorBoard notebook extension\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ./rampup_tensorboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap up, kill the Tensorboard process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 0:\n",
    "    os.system(\"kill -9 $(lsof -t -i:6006)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
